{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65730add-f712-4ca3-a27c-a04dc957f862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Complete [00h 07m 09s]\n",
      "val_accuracy: 0.8303571343421936\n",
      "\n",
      "Best val_accuracy So Far: 0.9285714030265808\n",
      "Total elapsed time: 00h 56m 38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/scratch/6706363/envs/myenv/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 72 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluation Results:\n",
      "Accuracy: 0.8643\n",
      "Precision: 0.8688\n",
      "Recall: 0.8643\n",
      "F1_score: 0.8600\n",
      "üíæ Saved results to results/BPIC2018/Transformer model/Baseline encoding/transformer_seq_2500.json\n",
      "\n",
      "üéâ All experiments completed!\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Force usage of GPU 3 before any TF import\n",
    "# --------------------------\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Force only GPU 3\n",
    "\n",
    "# --------------------------\n",
    "# Imports\n",
    "# --------------------------\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import keras_tuner as kt\n",
    "\n",
    "# --------------------------\n",
    "# Initialize environment\n",
    "# --------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "set_global_policy(\"mixed_float16\")\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# --------------------------\n",
    "# Detect GPU and set memory growth\n",
    "# --------------------------\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ GPU(s) detected: {[gpu.name for gpu in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è GPU memory growth setup failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected, running on CPU\")\n",
    "\n",
    "# Use only the single visible GPU (GPU 3) with OneDeviceStrategy\n",
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "print(f\"‚úÖ Number of GPUs available in strategy: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "# --------------------------\n",
    "# Load event log\n",
    "# --------------------------\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    event_log = pm4py.convert_to_dataframe(log)\n",
    "    return event_log\n",
    "\n",
    "event_log = import_xes(\"BPI Challenge 2018.xes\")\n",
    "df = event_log[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "df = df.sort_values(by=['org:resource', 'time:timestamp'])\n",
    "\n",
    "# --------------------------\n",
    "# Sequence creation\n",
    "# --------------------------\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "    for resource, resource_df in df.groupby('org:resource'):\n",
    "        activities = resource_df['concept:name'].values\n",
    "        if len(activities) >= prefix_length + 1:\n",
    "            sequences.append(activities[:prefix_length])\n",
    "            next_activities.append(activities[prefix_length])\n",
    "            resources.append(resource)\n",
    "    sequences_df = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    sequences_df['next_activity'] = next_activities\n",
    "    sequences_df['org:resource'] = resources\n",
    "    return sequences_df\n",
    "\n",
    "# --------------------------\n",
    "# Oversample rare classes proportionally\n",
    "# --------------------------\n",
    "def oversample_proportional(X, y):\n",
    "    counts = pd.Series(y).value_counts()\n",
    "    max_count = counts.max()\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls in counts.index:\n",
    "        cls_mask = (y == cls)\n",
    "        X_cls, y_cls = X[cls_mask], y[cls_mask]\n",
    "        n_repeat = int(np.ceil(max_count / len(y_cls)))\n",
    "        X_resampled.append(np.tile(X_cls, (n_repeat, 1)))\n",
    "        y_resampled.append(np.tile(y_cls, n_repeat))\n",
    "    X_bal = np.vstack(X_resampled)\n",
    "    y_bal = np.hstack(y_resampled)\n",
    "    return X_bal, y_bal\n",
    "\n",
    "# --------------------------\n",
    "# Transformer model builder\n",
    "# --------------------------\n",
    "def build_transformer_model(hp, prefix_length, num_classes):\n",
    "    d_model = hp.Choice(\"d_model\", [32, 64])\n",
    "    num_heads = hp.Choice(\"num_heads\", [2, 4])\n",
    "    num_layers = hp.Int(\"num_layers\", 1, 2)\n",
    "    dropout = 0.1\n",
    "\n",
    "    inputs = layers.Input(shape=(prefix_length,), dtype=tf.int32)\n",
    "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model)(inputs)\n",
    "    positions = tf.range(start=0, limit=prefix_length, delta=1)\n",
    "    pos_encoding = layers.Embedding(input_dim=prefix_length, output_dim=d_model)(positions)\n",
    "    x = layers.Lambda(lambda x: x + pos_encoding)(x)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        ffn = layers.Dense(d_model*4, activation=\"relu\")(x)\n",
    "        ffn = layers.Dense(d_model)(ffn)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --------------------------\n",
    "# Experiment Runner\n",
    "# --------------------------\n",
    "def run_experiment(prefix_length):\n",
    "    print(f\"\\nüöÄ Running Transformer experiment: sequence length = {prefix_length}\")\n",
    "\n",
    "    sequences_df = create_activity_sequences(df, prefix_length).reset_index(drop=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    activity_cols = [f\"activity_{i+1}\" for i in range(prefix_length)]\n",
    "    all_activities = sequences_df[activity_cols + ['next_activity']].values.flatten()\n",
    "    label_encoder.fit(all_activities)\n",
    "    for col in activity_cols + ['next_activity']:\n",
    "        sequences_df[col] = label_encoder.transform(sequences_df[col])\n",
    "\n",
    "    X = sequences_df[activity_cols].values.astype(np.int32)\n",
    "    y = sequences_df[\"next_activity\"].values.astype(np.int32)\n",
    "    X, y = oversample_proportional(X, y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    with strategy.scope():\n",
    "        tuner = kt.RandomSearch(\n",
    "            lambda hp: build_transformer_model(hp, prefix_length, num_classes),\n",
    "            objective='val_accuracy',\n",
    "            max_trials=15,\n",
    "            executions_per_trial=1,\n",
    "            directory='gpu_tuner',\n",
    "            project_name=f'transformer_seq_{prefix_length}',\n",
    "            overwrite=True\n",
    "        )\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    batch_size = 32\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            tuner.search(\n",
    "                X_train, y_train,\n",
    "                validation_split=0.2,\n",
    "                epochs=50,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=1\n",
    "            )\n",
    "            break\n",
    "        except tf.errors.ResourceExhaustedError:\n",
    "            print(f\"‚ö†Ô∏è Resource exhausted at batch_size={batch_size}. Reducing batch size.\")\n",
    "            batch_size = max(1, batch_size // 2)\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    y_pred = np.argmax(best_model.predict(X_test, batch_size=1, verbose=0), axis=1)\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_test, y_pred)),\n",
    "        \"precision\": float(precision_score(y_test, y_pred, average=\"weighted\", zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_test, y_pred, average=\"weighted\", zero_division=0)),\n",
    "        \"f1_score\": float(f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "\n",
    "    print(\"\\nüìä Evaluation Results:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k.capitalize()}: {v:.4f}\")\n",
    "\n",
    "    os.makedirs(\"results/BPIC2018/Transformer model/Baseline encoding\", exist_ok=True)\n",
    "    out_path = f\"results/BPIC2018/Transformer model/Baseline encoding/transformer_seq_{prefix_length}.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump({\"sequence_length\": prefix_length, \"best_hyperparameters\": best_hp.values, \"metrics\": metrics}, f, indent=4)\n",
    "    print(f\"üíæ Saved results to {out_path}\")\n",
    "\n",
    "# --------------------------\n",
    "# Run all experiments sequentially\n",
    "# --------------------------\n",
    "sequence_lengths = [2500]\n",
    "for seq_len in sequence_lengths:\n",
    "    run_experiment(seq_len)\n",
    "\n",
    "print(\"\\nüéâ All experiments completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f0bec1-1689-4425-abc3-939525630336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [00h 02m 34s]\n",
      "val_accuracy: 0.8831169009208679\n",
      "\n",
      "Best val_accuracy So Far: 0.9350649118423462\n",
      "Total elapsed time: 00h 25m 41s\n",
      "\n",
      "Search: Running Trial #7\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "32                |64                |d_model\n",
      "4                 |4                 |num_heads\n",
      "2                 |2                 |num_layers\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 20:29:19.205490: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.1382 - loss: 3.4352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 20:29:30.694841: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 461ms/step - accuracy: 0.2251 - loss: 3.1396 - val_accuracy: 0.3636 - val_loss: 2.6934\n",
      "Epoch 2/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 448ms/step - accuracy: 0.4454 - loss: 2.4474 - val_accuracy: 0.4221 - val_loss: 2.3733\n",
      "Epoch 3/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 447ms/step - accuracy: 0.5008 - loss: 2.2328 - val_accuracy: 0.5260 - val_loss: 2.2163\n",
      "Epoch 4/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 443ms/step - accuracy: 0.5726 - loss: 2.1020 - val_accuracy: 0.5260 - val_loss: 2.1037\n",
      "Epoch 5/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 446ms/step - accuracy: 0.5791 - loss: 2.0030 - val_accuracy: 0.6104 - val_loss: 2.0090\n",
      "Epoch 6/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 442ms/step - accuracy: 0.6183 - loss: 1.9185 - val_accuracy: 0.5974 - val_loss: 1.9237\n",
      "Epoch 7/50\n",
      "\u001b[1m15/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m2s\u001b[0m 421ms/step - accuracy: 0.6167 - loss: 1.8783"
     ]
    }
   ],
   "source": [
    "# Experiment 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import keras_tuner as kt\n",
    "\n",
    "# --------------------------\n",
    "# Initialize environment\n",
    "# --------------------------\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Use available GPU(s)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"‚úÖ Number of GPUs available: {len(gpus)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU found, using CPU.\")\n",
    "\n",
    "# --------------------------\n",
    "# Load event log\n",
    "# --------------------------\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    df = pm4py.convert_to_dataframe(log)\n",
    "    df = df[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "    df = df.sort_values(by=['org:resource', 'time:timestamp']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "event_log = import_xes(\"BPI Challenge 2018.xes\")\n",
    "\n",
    "# --------------------------\n",
    "# Resource-Activity diversity matrix\n",
    "# --------------------------\n",
    "def create_diversity_matrix(log):\n",
    "    activity_counts = log.pivot_table(\n",
    "        index='org:resource',\n",
    "        columns='concept:name',\n",
    "        aggfunc='size',\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    # Convert counts to binary (resource has done activity or not)\n",
    "    binary_matrix = activity_counts.copy()\n",
    "    binary_matrix.iloc[:, 1:] = (binary_matrix.iloc[:, 1:] > 0).astype(int)\n",
    "    return binary_matrix\n",
    "\n",
    "ra_matrix = create_diversity_matrix(event_log)\n",
    "\n",
    "# --------------------------\n",
    "# Sequence creation\n",
    "# --------------------------\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "    for resource, group in df.groupby('org:resource'):\n",
    "        acts = group['concept:name'].values\n",
    "        if len(acts) >= prefix_length + 1:\n",
    "            sequences.append(acts[:prefix_length])\n",
    "            next_activities.append(acts[prefix_length])\n",
    "            resources.append(resource)\n",
    "    df_seq = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    df_seq['next_activity'] = next_activities\n",
    "    df_seq['org:resource'] = resources\n",
    "    return df_seq\n",
    "\n",
    "# --------------------------\n",
    "# Oversample rare classes\n",
    "# --------------------------\n",
    "def oversample_proportional(X, y):\n",
    "    counts = pd.Series(y).value_counts()\n",
    "    max_count = counts.max()\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls in counts.index:\n",
    "        cls_mask = (y == cls)\n",
    "        X_cls, y_cls = X[cls_mask], y[cls_mask]\n",
    "        n_repeat = int(np.ceil(max_count / len(y_cls)))\n",
    "        X_resampled.append(np.tile(X_cls, (n_repeat, 1)))\n",
    "        y_resampled.append(np.tile(y_cls, n_repeat))\n",
    "    X_bal = np.vstack(X_resampled)\n",
    "    y_bal = np.hstack(y_resampled)\n",
    "    return X_bal, y_bal\n",
    "\n",
    "# --------------------------\n",
    "# Transformer model\n",
    "# --------------------------\n",
    "def build_transformer_model(hp, prefix_length, num_classes, num_extra_features):\n",
    "    d_model = hp.Choice(\"d_model\", [32, 64])\n",
    "    num_heads = hp.Choice(\"num_heads\", [2, 4])\n",
    "    num_layers = hp.Int(\"num_layers\", 1, 2)\n",
    "    dropout = 0.1\n",
    "\n",
    "    inputs = layers.Input(shape=(prefix_length + num_extra_features,), dtype=tf.float32)\n",
    "\n",
    "    # Split sequence vs extra RA features\n",
    "    activity_input = layers.Lambda(lambda x: tf.cast(x[:, :prefix_length], tf.int32))(inputs)\n",
    "    extra_input = layers.Lambda(lambda x: x[:, prefix_length:])(inputs)\n",
    "\n",
    "    # Embedding for activity sequence\n",
    "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model)(activity_input)\n",
    "    \n",
    "    # Positional encoding\n",
    "    positions = tf.range(start=0, limit=prefix_length, delta=1)\n",
    "    pos_encoding = layers.Embedding(input_dim=prefix_length, output_dim=d_model)(positions)\n",
    "    x = x + pos_encoding\n",
    "\n",
    "    # Transformer blocks\n",
    "    for _ in range(num_layers):\n",
    "        attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn)\n",
    "        ffn = layers.Dense(d_model*4, activation=\"relu\")(x)\n",
    "        ffn = layers.Dense(d_model)(ffn)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Concatenate()([x, extra_input])\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --------------------------\n",
    "# Run experiment\n",
    "# --------------------------\n",
    "def run_experiment(prefix_length, batch_size=32):\n",
    "    print(f\"\\nüöÄ Running experiment for sequence length = {prefix_length}\")\n",
    "\n",
    "    seq_df = create_activity_sequences(event_log, prefix_length)\n",
    "\n",
    "    # Merge RA binary features\n",
    "    ra_filtered = ra_matrix[ra_matrix['org:resource'].isin(seq_df['org:resource'])].reset_index(drop=True)\n",
    "    merged_df = pd.concat([seq_df.reset_index(drop=True), ra_filtered.iloc[:, 1:]], axis=1)\n",
    "\n",
    "    # Encode activities\n",
    "    activity_cols = [f\"activity_{i+1}\" for i in range(prefix_length)] + ['next_activity']\n",
    "    le = LabelEncoder()\n",
    "    all_acts = merged_df[activity_cols].values.flatten()\n",
    "    le.fit(all_acts)\n",
    "    for col in activity_cols:\n",
    "        merged_df[col] = le.transform(merged_df[col])\n",
    "\n",
    "    # Prepare X and y\n",
    "    extra_cols = ra_filtered.columns[1:].tolist()\n",
    "    X = merged_df[[f\"activity_{i+1}\" for i in range(prefix_length)] + extra_cols].values.astype(np.float32)\n",
    "    y = merged_df['next_activity'].values.astype(np.int32)\n",
    "\n",
    "    # Oversample\n",
    "    X, y = oversample_proportional(X, y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    num_classes = len(le.classes_)\n",
    "    num_extra_features = len(extra_cols)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    tuner = kt.RandomSearch(\n",
    "        lambda hp: build_transformer_model(hp, prefix_length, num_classes, num_extra_features),\n",
    "        objective=\"val_accuracy\",\n",
    "        max_trials=10,\n",
    "        executions_per_trial=1,\n",
    "        directory=\"gpu_tuner\",\n",
    "        project_name=f\"transformer_seq_{prefix_length}\",\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "    tuner.search(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=50,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = np.argmax(best_model.predict(X_test, batch_size=batch_size, verbose=0), axis=1)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "    }\n",
    "\n",
    "    print(\"\\nüìä Evaluation Results:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(\"results/BPIC2018/Transformer model/SCap\", exist_ok=True)\n",
    "    out_file = f\"results/BPIC2018/Transformer model/SCap/transformer_seq_{prefix_length}.json\"\n",
    "    with open(out_file, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"sequence_length\": prefix_length,\n",
    "            \"best_hyperparameters\": tuner.get_best_hyperparameters()[0].values,\n",
    "            \"metrics\": metrics\n",
    "        }, f, indent=4)\n",
    "    print(f\"üíæ Results saved to {out_file}\")\n",
    "\n",
    "# --------------------------\n",
    "# Run experiments\n",
    "# --------------------------\n",
    "sequence_lengths = [1000, 1200, 1400, 1500, 2000, 2500]\n",
    "for seq_len in sequence_lengths:\n",
    "    run_experiment(seq_len, batch_size=32)\n",
    "\n",
    "print(\"\\nüéâ All experiments completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e28ca-99f5-42e7-873d-59143a3e9ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
