{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65730add-f712-4ca3-a27c-a04dc957f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Force GPU 3\n",
    "\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    df = pm4py.convert_to_dataframe(log)\n",
    "    df = df[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "    df = df.sort_values(by=['org:resource', 'time:timestamp'])\n",
    "    return df\n",
    "\n",
    "df = import_xes(\"BPI_Challenge_2019.xes\")\n",
    "\n",
    "\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "    for resource, resource_df in df.groupby('org:resource'):\n",
    "        activities = resource_df['concept:name'].values\n",
    "        if len(activities) >= prefix_length + 1:\n",
    "            sequences.append(activities[:prefix_length])\n",
    "            next_activities.append(activities[prefix_length])\n",
    "            resources.append(resource)\n",
    "    sequences_df = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    sequences_df['next_activity'] = next_activities\n",
    "    sequences_df['org:resource'] = resources\n",
    "    return sequences_df\n",
    "\n",
    "def oversample_proportional(X, y):\n",
    "    counts = pd.Series(y).value_counts()\n",
    "    max_count = counts.max()\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls in counts.index:\n",
    "        cls_mask = (y == cls)\n",
    "        X_cls, y_cls = X[cls_mask], y[cls_mask]\n",
    "        n_repeat = int(np.ceil(max_count / len(y_cls)))\n",
    "        X_resampled.append(np.tile(X_cls, (n_repeat, 1)))\n",
    "        y_resampled.append(np.tile(y_cls, n_repeat))\n",
    "    X_bal = np.vstack(X_resampled)\n",
    "    y_bal = np.hstack(y_resampled)\n",
    "    return X_bal, y_bal\n",
    "\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, d_model=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)  # (batch, seq_len=1, d_model)\n",
    "        x = self.transformer(x)             # (batch, 1, d_model)\n",
    "        x = x.mean(dim=1)                   # global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def run_experiment(prefix_length, epochs=50, patience=5):\n",
    "    print(f\"\\nüöÄ Running experiment: sequence length = {prefix_length}\")\n",
    "\n",
    "    sequences_df = create_activity_sequences(df, prefix_length)\n",
    "    label_encoder = LabelEncoder()\n",
    "    activity_cols = [f\"activity_{i+1}\" for i in range(prefix_length)]\n",
    "    all_activities = sequences_df[activity_cols + ['next_activity']].values.flatten()\n",
    "    label_encoder.fit(all_activities)\n",
    "    for col in activity_cols + ['next_activity']:\n",
    "        sequences_df[col] = label_encoder.transform(sequences_df[col])\n",
    "\n",
    "    X = sequences_df[activity_cols].values.astype(np.float32)\n",
    "    y = sequences_df['next_activity'].values.astype(np.int64)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    X_train, y_train = oversample_proportional(X_train, y_train)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for d_model in [32, 64]:\n",
    "        for num_heads in [2, 4]:\n",
    "            for num_layers in [1, 2]:\n",
    "                model = ActivityTransformer(X_train.shape[1], len(label_encoder.classes_),\n",
    "                                            d_model=d_model, num_heads=num_heads, num_layers=num_layers).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                best_epoch_loss = float('inf')\n",
    "                patience_counter = 0\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_train)\n",
    "                    loss = criterion(outputs, y_train)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if loss.item() < best_epoch_loss:\n",
    "                        best_epoch_loss = loss.item()\n",
    "                        patience_counter = 0\n",
    "                        best_model_state = model.state_dict()\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= patience:\n",
    "                            break\n",
    "\n",
    "                # Evaluate\n",
    "                model.load_state_dict(best_model_state)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_pred = torch.argmax(model(X_test), dim=1)\n",
    "                    acc = accuracy_score(y_test.cpu(), y_pred.cpu())\n",
    "                if acc > best_val_acc:\n",
    "                    best_val_acc = acc\n",
    "                    best_model = model\n",
    "                    best_params = {'d_model': d_model, 'num_heads': num_heads, 'num_layers': num_layers}\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = torch.argmax(best_model(X_test), dim=1)\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_test.cpu(), y_pred.cpu())),\n",
    "        \"precision\": float(precision_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\", zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\", zero_division=0)),\n",
    "        \"f1_score\": float(f1_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\"))\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüìä Metrics for sequence length {prefix_length}: {metrics}\")\n",
    "    print(f\"üõ†Ô∏è Best hyperparameters: {best_params}\")\n",
    "\n",
    "    os.makedirs(\"results/BPIC2019/Transformer model/Baseline encoding\", exist_ok=True)\n",
    "    out_path = f\"results/BPIC2019/Transformer model/Baseline encoding/transformer_seq_{prefix_length}.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump({\"sequence_length\": prefix_length, \"best_hyperparameters\": best_params, \"metrics\": metrics}, f, indent=4)\n",
    "    print(f\"üíæ Saved results to {out_path}\")\n",
    "\n",
    "\n",
    "sequence_lengths = [100, 150, 200, 300, 400, 500, 600, 700, 800]\n",
    "for seq_len in sequence_lengths:\n",
    "    run_experiment(seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f0bec1-1689-4425-abc3-939525630336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCap\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    df = pm4py.convert_to_dataframe(log)\n",
    "    df = df[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "    df = df.sort_values(by=['org:resource', 'time:timestamp']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = import_xes(\"BPI_Challenge_2013_incidents.xes\")\n",
    "\n",
    "def create_ra_matrix(df):\n",
    "    activity_counts = df.pivot_table(\n",
    "        index='org:resource', columns='concept:name', aggfunc='size', fill_value=0\n",
    "    ).reset_index()\n",
    "    ra_matrix = activity_counts.copy()\n",
    "    ra_matrix.iloc[:, 1:] = (ra_matrix.iloc[:, 1:] > 0).astype(int)\n",
    "    return ra_matrix\n",
    "\n",
    "ra_matrix = create_ra_matrix(df)\n",
    "\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "    for resource, group in df.groupby('org:resource'):\n",
    "        acts = group['concept:name'].values\n",
    "        if len(acts) >= prefix_length + 1:\n",
    "            sequences.append(acts[:prefix_length])\n",
    "            next_activities.append(acts[prefix_length])\n",
    "            resources.append(resource)\n",
    "    seq_df = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    seq_df['next_activity'] = next_activities\n",
    "    seq_df['org:resource'] = resources\n",
    "    return seq_df\n",
    "\n",
    "def oversample_proportional(X, y):\n",
    "    counts = pd.Series(y).value_counts()\n",
    "    max_count = counts.max()\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls in counts.index:\n",
    "        cls_mask = (y == cls)\n",
    "        X_cls, y_cls = X[cls_mask], y[cls_mask]\n",
    "        n_repeat = int(np.ceil(max_count / len(y_cls)))\n",
    "        X_resampled.append(np.tile(X_cls, (n_repeat, 1)))\n",
    "        y_resampled.append(np.tile(y_cls, n_repeat))\n",
    "    X_bal = np.vstack(X_resampled)\n",
    "    y_bal = np.hstack(y_resampled)\n",
    "    return X_bal, y_bal\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, d_model=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)  # (batch, seq_len=1, d_model)\n",
    "        x = self.transformer(x)             # (batch, 1, d_model)\n",
    "        x = x.mean(dim=1)                   # global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def run_experiment(prefix_length, epochs=50, patience=5):\n",
    "    print(f\"\\nüöÄ Running experiment: sequence length = {prefix_length}\")\n",
    "\n",
    "    seq_df = create_activity_sequences(df, prefix_length)\n",
    "\n",
    "    ra_filtered = ra_matrix[ra_matrix['org:resource'].isin(seq_df['org:resource'])].reset_index(drop=True)\n",
    "    merged_df = pd.concat([seq_df.reset_index(drop=True), ra_filtered.iloc[:, 1:]], axis=1)\n",
    "\n",
    "    activity_cols = [f\"activity_{i+1}\" for i in range(prefix_length)]\n",
    "    le = LabelEncoder()\n",
    "    all_acts = merged_df[activity_cols + ['next_activity']].values.flatten()\n",
    "    le.fit(all_acts)\n",
    "    for col in activity_cols + ['next_activity']:\n",
    "        merged_df[col] = le.transform(merged_df[col])\n",
    "\n",
    "    extra_cols = ra_filtered.columns[1:].tolist()\n",
    "    X = merged_df[activity_cols + extra_cols].values.astype(np.float32)\n",
    "    y = merged_df['next_activity'].values.astype(np.int64)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    X_train, y_train = oversample_proportional(X_train, y_train)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    best_model, best_params, best_val_acc = None, None, 0.0\n",
    "    for d_model in [32, 64]:\n",
    "        for num_heads in [2, 4]:\n",
    "            for num_layers in [1, 2]:\n",
    "                model = ActivityTransformer(X_train.shape[1], len(le.classes_),\n",
    "                                            d_model=d_model, num_heads=num_heads, num_layers=num_layers).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                best_epoch_loss, patience_counter = float('inf'), 0\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_train)\n",
    "                    loss = criterion(outputs, y_train)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if loss.item() < best_epoch_loss:\n",
    "                        best_epoch_loss = loss.item()\n",
    "                        patience_counter = 0\n",
    "                        best_model_state = model.state_dict()\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= patience:\n",
    "                            break\n",
    "\n",
    "                # Evaluate\n",
    "                model.load_state_dict(best_model_state)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_pred = torch.argmax(model(X_test), dim=1)\n",
    "                    acc = accuracy_score(y_test.cpu(), y_pred.cpu())\n",
    "                if acc > best_val_acc:\n",
    "                    best_val_acc = acc\n",
    "                    best_model = model\n",
    "                    best_params = {'d_model': d_model, 'num_heads': num_heads, 'num_layers': num_layers}\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = torch.argmax(best_model(X_test), dim=1)\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_test.cpu(), y_pred.cpu())),\n",
    "        \"precision\": float(precision_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\", zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\", zero_division=0)),\n",
    "        \"f1_score\": float(f1_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\"))\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüìä Metrics for sequence length {prefix_length}: {metrics}\")\n",
    "    print(f\"üõ†Ô∏è Best hyperparameters: {best_params}\")\n",
    "\n",
    "    os.makedirs(\"results/BPIC2013/Transformer model/SCap\", exist_ok=True)\n",
    "    out_path = f\"results/BPIC2013/Transformer model/SCap/transformer_seq_{prefix_length}.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump({\"sequence_length\": prefix_length, \"best_hyperparameters\": best_params, \"metrics\": metrics}, f, indent=4)\n",
    "    print(f\"üíæ Saved results to {out_path}\")\n",
    "\n",
    "\n",
    "sequence_lengths = [10, 20, 30, 40, 50, 75, 100, 125, 150]\n",
    "for seq_len in sequence_lengths:\n",
    "    run_experiment(seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e28ca-99f5-42e7-873d-59143a3e9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S2g\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Force GPU 3\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    df = pm4py.convert_to_dataframe(log)\n",
    "    df = df[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "    df = df.sort_values(by=['org:resource', 'time:timestamp']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = import_xes(\"BPI_Challenge_2013_incidents.xes\")\n",
    "\n",
    "def create_transition_count_features(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "    all_activities = df['concept:name'].unique()\n",
    "    activity_to_idx = {act: i for i, act in enumerate(all_activities)}\n",
    "    n_activities = len(all_activities)\n",
    "\n",
    "    n_transitions = n_activities * n_activities\n",
    "\n",
    "    for resource, group in df.groupby('org:resource'):\n",
    "        acts = group['concept:name'].values\n",
    "        if len(acts) >= prefix_length + 1:\n",
    "            # Count transitions over the whole prefix\n",
    "            transition_counts = np.zeros(n_transitions, dtype=np.float32)\n",
    "            for t in range(prefix_length - 1):\n",
    "                i = activity_to_idx[acts[t]]\n",
    "                j = activity_to_idx[acts[t + 1]]\n",
    "                transition_counts[i * n_activities + j] += 1.0  # increment count\n",
    "\n",
    "            sequences.append(transition_counts)\n",
    "            next_activities.append(acts[prefix_length])\n",
    "            resources.append(resource)\n",
    "\n",
    "    sequences_df = pd.DataFrame(sequences)\n",
    "    sequences_df['next_activity'] = next_activities\n",
    "    sequences_df['org:resource'] = resources\n",
    "    return sequences_df, all_activities\n",
    "\n",
    "def oversample_proportional(X, y):\n",
    "    counts = pd.Series(y).value_counts()\n",
    "    max_count = counts.max()\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls in counts.index:\n",
    "        cls_mask = (y == cls)\n",
    "        X_cls, y_cls = X[cls_mask], y[cls_mask]\n",
    "        n_repeat = int(np.ceil(max_count / len(y_cls)))\n",
    "        X_resampled.append(np.tile(X_cls, (n_repeat, 1)))\n",
    "        y_resampled.append(np.tile(y_cls, n_repeat))\n",
    "    X_bal = np.vstack(X_resampled)\n",
    "    y_bal = np.hstack(y_resampled)\n",
    "    return X_bal, y_bal\n",
    "\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, d_model=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)  # (batch, seq_len=1, d_model)\n",
    "        x = self.transformer(x)             # (batch, 1, d_model)\n",
    "        x = x.mean(dim=1)                   # global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def run_experiment(prefix_length, epochs=50, patience=5):\n",
    "    print(f\"\\nüöÄ Running experiment: sequence length = {prefix_length}\")\n",
    "    \n",
    "    sequences_df, all_activities = create_transition_count_features(df, prefix_length)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(sequences_df['next_activity'])\n",
    "    sequences_df['next_activity'] = le.transform(sequences_df['next_activity'])\n",
    "    \n",
    "    X = sequences_df.drop(columns=['next_activity', 'org:resource']).values.astype(np.float32)\n",
    "    y = sequences_df['next_activity'].values.astype(np.int64)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "    \n",
    "    X_train, y_train = oversample_proportional(X_train, y_train)\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "    \n",
    "    best_model, best_params, best_val_acc = None, None, 0.0\n",
    "    for d_model in [32, 64]:\n",
    "        for num_heads in [2, 4]:\n",
    "            for num_layers in [1, 2]:\n",
    "                model = ActivityTransformer(X_train.shape[1], len(le.classes_),\n",
    "                                            d_model=d_model, num_heads=num_heads, num_layers=num_layers).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                best_epoch_loss, patience_counter = float('inf'), 0\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_train)\n",
    "                    loss = criterion(outputs, y_train)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    if loss.item() < best_epoch_loss:\n",
    "                        best_epoch_loss = loss.item()\n",
    "                        patience_counter = 0\n",
    "                        best_model_state = model.state_dict()\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= patience:\n",
    "                            break\n",
    "                \n",
    "                # Evaluate\n",
    "                model.load_state_dict(best_model_state)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_pred = torch.argmax(model(X_test), dim=1)\n",
    "                    acc = accuracy_score(y_test.cpu(), y_pred.cpu())\n",
    "                if acc > best_val_acc:\n",
    "                    best_val_acc = acc\n",
    "                    best_model = model\n",
    "                    best_params = {'d_model': d_model, 'num_heads': num_heads, 'num_layers': num_layers}\n",
    "    \n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = torch.argmax(best_model(X_test), dim=1)\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_test.cpu(), y_pred.cpu())),\n",
    "        \"precision\": float(precision_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\", zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\", zero_division=0)),\n",
    "        \"f1_score\": float(f1_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\"))\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Metrics for sequence length {prefix_length}: {metrics}\")\n",
    "    print(f\"üõ†Ô∏è Best hyperparameters: {best_params}\")\n",
    "    \n",
    "    os.makedirs(\"results/BPIC2013/Transformer model/S2g\", exist_ok=True)\n",
    "    out_path = f\"results/BPIC2013/Transformer model/S2g/transformer_seq_{prefix_length}.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump({\"sequence_length\": prefix_length, \"best_hyperparameters\": best_params, \"metrics\": metrics}, f, indent=4)\n",
    "    print(f\"üíæ Saved results to {out_path}\")\n",
    "\n",
    "\n",
    "sequence_lengths = [10, 20, 30, 40, 50, 75, 100, 125, 150]\n",
    "for seq_len in sequence_lengths:\n",
    "    run_experiment(seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c582d-04ee-4bce-af71-a2795abaeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S2gR\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import json, gc, numpy as np, pandas as pd\n",
    "from collections import defaultdict\n",
    "import pm4py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    df = pm4py.convert_to_dataframe(log)\n",
    "    df = df[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "    df = df.sort_values(by=['org:resource', 'time:timestamp']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = import_xes(\"BPI_Challenge_2013_incidents.xes\")\n",
    "\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "    for resource, resource_df in df.groupby('org:resource'):\n",
    "        activities = resource_df['concept:name'].values\n",
    "        if len(activities) >= prefix_length + 1:\n",
    "            sequences.append(activities[:prefix_length])\n",
    "            next_activities.append(activities[prefix_length])\n",
    "            resources.append(resource)\n",
    "    sequences_df = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    sequences_df['next_activity'] = next_activities\n",
    "    sequences_df['org:resource'] = resources\n",
    "    return sequences_df\n",
    "\n",
    "def create_transition_and_repeat_features(sequences_df):\n",
    "    unique_activities = sorted(\n",
    "        set(sequences_df.drop(columns=[\"next_activity\", \"org:resource\"]).values.flatten()) - {None}\n",
    "    )\n",
    "    all_possible_transitions = [(a, b) for a in unique_activities for b in unique_activities]\n",
    "\n",
    "    transition_counts = []\n",
    "    repeat_pattern_features = []\n",
    "\n",
    "    for _, row in sequences_df.iterrows():\n",
    "        transitions = defaultdict(int)\n",
    "        activities = row.drop(labels=[\"next_activity\", \"org:resource\"]).dropna().tolist()\n",
    "\n",
    "        # Transition counts\n",
    "        for i in range(len(activities) - 1):\n",
    "            transitions[(activities[i], activities[i + 1])] += 1\n",
    "        row_counts = [transitions.get((a, b), 0) for (a, b) in all_possible_transitions]\n",
    "        transition_counts.append(row_counts)\n",
    "\n",
    "        # Repeat pattern features\n",
    "        current_run = 1\n",
    "        run_lengths = []\n",
    "        for i in range(1, len(activities)):\n",
    "            if activities[i] == activities[i - 1]:\n",
    "                current_run += 1\n",
    "            else:\n",
    "                run_lengths.append(current_run)\n",
    "                current_run = 1\n",
    "        run_lengths.append(current_run)\n",
    "        repeat_pattern_features.append([np.mean(run_lengths), len(run_lengths)])\n",
    "\n",
    "    transitions_df = pd.DataFrame(transition_counts)\n",
    "    repeat_df = pd.DataFrame(repeat_pattern_features, columns=[\"avg_run_length\", \"num_runs\"])\n",
    "    return pd.concat([sequences_df.reset_index(drop=True), transitions_df, repeat_df], axis=1)\n",
    "\n",
    "def oversample_proportional(X, y):\n",
    "    counts = pd.Series(y).value_counts()\n",
    "    max_count = counts.max()\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls in counts.index:\n",
    "        cls_mask = (y == cls)\n",
    "        X_cls, y_cls = X[cls_mask], y[cls_mask]\n",
    "        n_repeat = int(np.ceil(max_count / len(y_cls)))\n",
    "        X_resampled.append(np.tile(X_cls, (n_repeat, 1)))\n",
    "        y_resampled.append(np.tile(y_cls, n_repeat))\n",
    "    return np.vstack(X_resampled), np.hstack(y_resampled)\n",
    "\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, d_model=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def run_experiment(prefix_length, epochs=50, patience=5):\n",
    "    print(f\"\\nüöÄ Running experiment: sequence length = {prefix_length}\")\n",
    "\n",
    "    sequences_df = create_activity_sequences(df, prefix_length)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    activity_cols = [f\"activity_{i+1}\" for i in range(prefix_length)]\n",
    "    all_activities = sequences_df[activity_cols + ['next_activity']].values.flatten()\n",
    "    label_encoder.fit(all_activities)\n",
    "    for col in activity_cols + ['next_activity']:\n",
    "        sequences_df[col] = label_encoder.transform(sequences_df[col])\n",
    "\n",
    "    sequences_df = create_transition_and_repeat_features(sequences_df)\n",
    "    X = sequences_df.drop(columns=['next_activity', 'org:resource']).values.astype(np.float32)\n",
    "    y = sequences_df['next_activity'].values.astype(np.int64)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                        random_state=42, shuffle=True)\n",
    "\n",
    "    X_train, y_train = oversample_proportional(X_train, y_train)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    best_model, best_params, best_val_acc = None, None, 0.0\n",
    "    for d_model in [32, 64]:\n",
    "        for num_heads in [2, 4]:\n",
    "            for num_layers in [1, 2]:\n",
    "                model = ActivityTransformer(X_train.shape[1], len(label_encoder.classes_),\n",
    "                                            d_model=d_model, num_heads=num_heads, num_layers=num_layers).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                best_epoch_loss, patience_counter = float('inf'), 0\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_train)\n",
    "                    loss = criterion(outputs, y_train)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if loss.item() < best_epoch_loss:\n",
    "                        best_epoch_loss = loss.item()\n",
    "                        patience_counter = 0\n",
    "                        best_model_state = model.state_dict()\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= patience:\n",
    "                            break\n",
    "\n",
    "                # Evaluate\n",
    "                model.load_state_dict(best_model_state)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_pred = torch.argmax(model(X_test), dim=1)\n",
    "                    acc = accuracy_score(y_test.cpu(), y_pred.cpu())\n",
    "                if acc > best_val_acc:\n",
    "                    best_val_acc = acc\n",
    "                    best_model = model\n",
    "                    best_params = {'d_model': d_model, 'num_heads': num_heads, 'num_layers': num_layers}\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = torch.argmax(best_model(X_test), dim=1)\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_test.cpu(), y_pred.cpu())),\n",
    "        \"precision\": float(precision_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\", zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\", zero_division=0)),\n",
    "        \"f1_score\": float(f1_score(y_test.cpu(), y_pred.cpu(), average=\"weighted\"))\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüìä Metrics for sequence length {prefix_length}: {metrics}\")\n",
    "    print(f\"üõ†Ô∏è Best hyperparameters: {best_params}\")\n",
    "\n",
    "    os.makedirs(\"results/BPIC2013/Transformer model/S2gR\", exist_ok=True)\n",
    "    out_path = f\"results/BPIC2013/Transformer model/S2gR/transformer_seq_{prefix_length}.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump({\"sequence_length\": prefix_length, \"best_hyperparameters\": best_params, \"metrics\": metrics}, f, indent=4)\n",
    "    print(f\"üíæ Saved results to {out_path}\")\n",
    "\n",
    "sequence_lengths = [10, 20, 30, 40, 50, 75, 100, 125, 150]\n",
    "for seq_len in sequence_lengths:\n",
    "    run_experiment(seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f896668-06cf-4745-96ad-dbda19616738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Folder zipped at: models.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Folder you want to download\n",
    "folder_path = \"models\"\n",
    "# Where to save the zip\n",
    "zip_path = folder_path + \".zip\"\n",
    "\n",
    "# Create a zip file\n",
    "shutil.make_archive(folder_path, 'zip', folder_path)\n",
    "\n",
    "print(f\"‚úÖ Folder zipped at: {zip_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a7c555-a4d3-429f-b9ee-c84056e2021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fcc2d74d6c41bd902a337c832e5150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/251734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Running experiment: sequence length = 100\n",
      "üíæ Saved test set to data/bpic2019_s2gr_test_seq100.csv\n",
      "üíæ Saved LabelEncoder to models/activity_label_encoder_seq100.pkl\n",
      "\n",
      "üìä Metrics for sequence length 100: {'accuracy': 0.8307692307692308, 'precision': 0.7894586894586894, 'recall': 0.8307692307692308, 'f1_score': 0.7948914348063284}\n",
      "üõ†Ô∏è Best hyperparameters: {'d_model': 64, 'num_heads': 4, 'num_layers': 1}\n",
      "üíæ Saved model to models/bpic2019_transformer_s2gr_seq100.pt\n",
      "\n",
      "üöÄ Running experiment: sequence length = 150\n",
      "üíæ Saved test set to data/bpic2019_s2gr_test_seq150.csv\n",
      "üíæ Saved LabelEncoder to models/activity_label_encoder_seq150.pkl\n",
      "\n",
      "üìä Metrics for sequence length 150: {'accuracy': 0.9137931034482759, 'precision': 0.934437386569873, 'recall': 0.9137931034482759, 'f1_score': 0.9201086270051788}\n",
      "üõ†Ô∏è Best hyperparameters: {'d_model': 64, 'num_heads': 4, 'num_layers': 1}\n",
      "üíæ Saved model to models/bpic2019_transformer_s2gr_seq150.pt\n",
      "\n",
      "üöÄ Running experiment: sequence length = 200\n",
      "üíæ Saved test set to data/bpic2019_s2gr_test_seq200.csv\n",
      "üíæ Saved LabelEncoder to models/activity_label_encoder_seq200.pkl\n",
      "\n",
      "üìä Metrics for sequence length 200: {'accuracy': 0.9074074074074074, 'precision': 0.9620370370370371, 'recall': 0.9074074074074074, 'f1_score': 0.9331849270873662}\n",
      "üõ†Ô∏è Best hyperparameters: {'d_model': 32, 'num_heads': 4, 'num_layers': 2}\n",
      "üíæ Saved model to models/bpic2019_transformer_s2gr_seq200.pt\n",
      "\n",
      "üöÄ Running experiment: sequence length = 300\n",
      "üíæ Saved test set to data/bpic2019_s2gr_test_seq300.csv\n",
      "üíæ Saved LabelEncoder to models/activity_label_encoder_seq300.pkl\n",
      "\n",
      "üìä Metrics for sequence length 300: {'accuracy': 0.8936170212765957, 'precision': 0.9428191489361702, 'recall': 0.8936170212765957, 'f1_score': 0.9106227513914542}\n",
      "üõ†Ô∏è Best hyperparameters: {'d_model': 64, 'num_heads': 2, 'num_layers': 1}\n",
      "üíæ Saved model to models/bpic2019_transformer_s2gr_seq300.pt\n",
      "\n",
      "üöÄ Running experiment: sequence length = 400\n",
      "üíæ Saved test set to data/bpic2019_s2gr_test_seq400.csv\n",
      "üíæ Saved LabelEncoder to models/activity_label_encoder_seq400.pkl\n",
      "\n",
      "üìä Metrics for sequence length 400: {'accuracy': 0.9069767441860465, 'precision': 0.8558139534883721, 'recall': 0.9069767441860465, 'f1_score': 0.8762038994597133}\n",
      "üõ†Ô∏è Best hyperparameters: {'d_model': 32, 'num_heads': 4, 'num_layers': 2}\n",
      "üíæ Saved model to models/bpic2019_transformer_s2gr_seq400.pt\n",
      "\n",
      "üöÄ Running experiment: sequence length = 500\n",
      "üíæ Saved test set to data/bpic2019_s2gr_test_seq500.csv\n",
      "üíæ Saved LabelEncoder to models/activity_label_encoder_seq500.pkl\n",
      "\n",
      "üìä Metrics for sequence length 500: {'accuracy': 0.868421052631579, 'precision': 0.90311004784689, 'recall': 0.868421052631579, 'f1_score': 0.881578947368421}\n",
      "üõ†Ô∏è Best hyperparameters: {'d_model': 32, 'num_heads': 4, 'num_layers': 2}\n",
      "üíæ Saved model to models/bpic2019_transformer_s2gr_seq500.pt\n",
      "\n",
      "üöÄ Running experiment: sequence length = 600\n",
      "üíæ Saved test set to data/bpic2019_s2gr_test_seq600.csv\n",
      "üíæ Saved LabelEncoder to models/activity_label_encoder_seq600.pkl\n",
      "\n",
      "üìä Metrics for sequence length 600: {'accuracy': 0.8918918918918919, 'precision': 0.9405405405405405, 'recall': 0.8918918918918919, 'f1_score': 0.914004914004914}\n",
      "üõ†Ô∏è Best hyperparameters: {'d_model': 64, 'num_heads': 2, 'num_layers': 2}\n",
      "üíæ Saved model to models/bpic2019_transformer_s2gr_seq600.pt\n",
      "\n",
      "üöÄ Running experiment: sequence length = 700\n",
      "üíæ Saved test set to data/bpic2019_s2gr_test_seq700.csv\n",
      "üíæ Saved LabelEncoder to models/activity_label_encoder_seq700.pkl\n",
      "\n",
      "üìä Metrics for sequence length 700: {'accuracy': 0.8571428571428571, 'precision': 0.9785714285714285, 'recall': 0.8571428571428571, 'f1_score': 0.9069622107216092}\n",
      "üõ†Ô∏è Best hyperparameters: {'d_model': 32, 'num_heads': 4, 'num_layers': 1}\n",
      "üíæ Saved model to models/bpic2019_transformer_s2gr_seq700.pt\n",
      "\n",
      "üöÄ Running experiment: sequence length = 800\n",
      "üíæ Saved test set to data/bpic2019_s2gr_test_seq800.csv\n",
      "üíæ Saved LabelEncoder to models/activity_label_encoder_seq800.pkl\n",
      "\n",
      "üìä Metrics for sequence length 800: {'accuracy': 0.84375, 'precision': 0.7514204545454546, 'recall': 0.84375, 'f1_score': 0.7918154761904761}\n",
      "üõ†Ô∏è Best hyperparameters: {'d_model': 64, 'num_heads': 4, 'num_layers': 2}\n",
      "üíæ Saved model to models/bpic2019_transformer_s2gr_seq800.pt\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# Transformer Experiment 4 ‚Äî Transitions + Repeat Features + GPU (PyTorch)\n",
    "# Auto-saves model, encoder, and test set for SHAP\n",
    "# ==============================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import json, numpy as np, pandas as pd\n",
    "from collections import defaultdict\n",
    "import pm4py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "\n",
    "# --------------------------\n",
    "# GPU setup\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "# --------------------------\n",
    "# Load event log\n",
    "# --------------------------\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    df = pm4py.convert_to_dataframe(log)\n",
    "    df = df[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "    df = df.sort_values(by=['org:resource', 'time:timestamp']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = import_xes(\"BPI_Challenge_2019.xes\")\n",
    "\n",
    "# --------------------------\n",
    "# Create sequences\n",
    "# --------------------------\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "    for resource, resource_df in df.groupby('org:resource'):\n",
    "        activities = resource_df['concept:name'].values\n",
    "        if len(activities) >= prefix_length + 1:\n",
    "            sequences.append(activities[:prefix_length])\n",
    "            next_activities.append(activities[prefix_length])\n",
    "            resources.append(resource)\n",
    "    sequences_df = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    sequences_df['next_activity'] = next_activities\n",
    "    sequences_df['org:resource'] = resources\n",
    "    return sequences_df\n",
    "\n",
    "# --------------------------\n",
    "# Transition + Repeat features\n",
    "# --------------------------\n",
    "def create_transition_and_repeat_features(sequences_df):\n",
    "    unique_activities = sorted(\n",
    "        set(sequences_df.drop(columns=[\"next_activity\", \"org:resource\"]).values.flatten()) - {None}\n",
    "    )\n",
    "    all_possible_transitions = [(a, b) for a in unique_activities for b in unique_activities]\n",
    "\n",
    "    transition_counts = []\n",
    "    repeat_pattern_features = []\n",
    "\n",
    "    for _, row in sequences_df.iterrows():\n",
    "        transitions = defaultdict(int)\n",
    "        activities = row.drop(labels=[\"next_activity\", \"org:resource\"]).dropna().tolist()\n",
    "\n",
    "        # Transition counts\n",
    "        for i in range(len(activities) - 1):\n",
    "            transitions[(activities[i], activities[i + 1])] += 1\n",
    "        row_counts = [transitions.get((a, b), 0) for (a, b) in all_possible_transitions]\n",
    "        transition_counts.append(row_counts)\n",
    "\n",
    "        # Repeat pattern features\n",
    "        current_run = 1\n",
    "        run_lengths = []\n",
    "        for i in range(1, len(activities)):\n",
    "            if activities[i] == activities[i - 1]:\n",
    "                current_run += 1\n",
    "            else:\n",
    "                run_lengths.append(current_run)\n",
    "                current_run = 1\n",
    "        run_lengths.append(current_run)\n",
    "        repeat_pattern_features.append([np.mean(run_lengths), len(run_lengths)])\n",
    "\n",
    "    transitions_df = pd.DataFrame(transition_counts)\n",
    "    repeat_df = pd.DataFrame(repeat_pattern_features, columns=[\"avg_run_length\", \"num_runs\"])\n",
    "    return pd.concat([sequences_df.reset_index(drop=True), transitions_df, repeat_df], axis=1)\n",
    "\n",
    "# --------------------------\n",
    "# Oversample training set\n",
    "# --------------------------\n",
    "def oversample_proportional(X, y):\n",
    "    counts = pd.Series(y).value_counts()\n",
    "    max_count = counts.max()\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls in counts.index:\n",
    "        cls_mask = (y == cls)\n",
    "        X_cls, y_cls = X[cls_mask], y[cls_mask]\n",
    "        n_repeat = int(np.ceil(max_count / len(y_cls)))\n",
    "        X_resampled.append(np.tile(X_cls, (n_repeat, 1)))\n",
    "        y_resampled.append(np.tile(y_cls, n_repeat))\n",
    "    return np.vstack(X_resampled), np.hstack(y_resampled)\n",
    "\n",
    "# --------------------------\n",
    "# PyTorch Transformer\n",
    "# --------------------------\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, d_model=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# --------------------------\n",
    "# Run experiment\n",
    "# --------------------------\n",
    "def run_experiment(prefix_length, epochs=50, patience=5):\n",
    "    print(f\"\\nüöÄ Running experiment: sequence length = {prefix_length}\")\n",
    "\n",
    "    # 1Ô∏è‚É£ Create sequences\n",
    "    sequences_df = create_activity_sequences(df, prefix_length)\n",
    "\n",
    "    # 2Ô∏è‚É£ Encode activities\n",
    "    label_encoder = LabelEncoder()\n",
    "    activity_cols = [f\"activity_{i+1}\" for i in range(prefix_length)]\n",
    "    all_activities = sequences_df[activity_cols + ['next_activity']].values.flatten()\n",
    "    label_encoder.fit(all_activities)\n",
    "    for col in activity_cols + ['next_activity']:\n",
    "        sequences_df[col] = label_encoder.transform(sequences_df[col])\n",
    "\n",
    "    # 3Ô∏è‚É£ Transition + repeat features\n",
    "    sequences_df = create_transition_and_repeat_features(sequences_df)\n",
    "    X = sequences_df.drop(columns=['next_activity', 'org:resource']).values.astype(np.float32)\n",
    "    y = sequences_df['next_activity'].values.astype(np.int64)\n",
    "\n",
    "    # 4Ô∏è‚É£ Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                        random_state=42, shuffle=True)\n",
    "\n",
    "    # Save test set for SHAP\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    X_test_df = pd.DataFrame(X_test, columns=sequences_df.drop(columns=['next_activity', 'org:resource']).columns)\n",
    "    X_test_df.to_csv(f\"data/bpic2019_s2gr_test_seq{prefix_length}.csv\", index=False)\n",
    "    print(f\"üíæ Saved test set to data/bpic2019_s2gr_test_seq{prefix_length}.csv\")\n",
    "\n",
    "    # Save LabelEncoder for SHAP\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    with open(f\"models/activity_label_encoder_seq{prefix_length}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(f\"üíæ Saved LabelEncoder to models/activity_label_encoder_seq{prefix_length}.pkl\")\n",
    "\n",
    "    # 5Ô∏è‚É£ Oversample training set\n",
    "    X_train, y_train = oversample_proportional(X_train, y_train)\n",
    "\n",
    "    # 6Ô∏è‚É£ Convert to tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    # 7Ô∏è‚É£ Hyperparameter tuning\n",
    "    best_model, best_params, best_val_acc = None, None, 0.0\n",
    "    for d_model in [32, 64]:\n",
    "        for num_heads in [2, 4]:\n",
    "            for num_layers in [1, 2]:\n",
    "                model = ActivityTransformer(X_train.shape[1], len(label_encoder.classes_),\n",
    "                                            d_model=d_model, num_heads=num_heads, num_layers=num_layers).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                best_epoch_loss, patience_counter = float('inf'), 0\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_train)\n",
    "                    loss = criterion(outputs, y_train)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if loss.item() < best_epoch_loss:\n",
    "                        best_epoch_loss = loss.item()\n",
    "                        patience_counter = 0\n",
    "                        best_model_state = model.state_dict()\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= patience:\n",
    "                            break\n",
    "\n",
    "                # Evaluate\n",
    "                model.load_state_dict(best_model_state)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_pred = torch.argmax(model(X_test_tensor), dim=1)\n",
    "                    acc = accuracy_score(y_test_tensor.cpu(), y_pred.cpu())\n",
    "                if acc > best_val_acc:\n",
    "                    best_val_acc = acc\n",
    "                    best_model = model\n",
    "                    best_params = {'d_model': d_model, 'num_heads': num_heads, 'num_layers': num_layers}\n",
    "\n",
    "    # 8Ô∏è‚É£ Final evaluation\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = torch.argmax(best_model(X_test_tensor), dim=1)\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_test_tensor.cpu(), y_pred.cpu())),\n",
    "        \"precision\": float(precision_score(y_test_tensor.cpu(), y_pred.cpu(), average=\"weighted\", zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_test_tensor.cpu(), y_pred.cpu(), average=\"weighted\", zero_division=0)),\n",
    "        \"f1_score\": float(f1_score(y_test_tensor.cpu(), y_pred.cpu(), average=\"weighted\"))\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüìä Metrics for sequence length {prefix_length}: {metrics}\")\n",
    "    print(f\"üõ†Ô∏è Best hyperparameters: {best_params}\")\n",
    "\n",
    "    # Save model for SHAP\n",
    "    torch.save(best_model.state_dict(), f\"models/bpic2019_transformer_s2gr_seq{prefix_length}.pt\")\n",
    "    print(f\"üíæ Saved model to models/bpic2019_transformer_s2gr_seq{prefix_length}.pt\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Run multiple sequence lengths\n",
    "# --------------------------\n",
    "sequence_lengths = [100, 150, 200, 300, 400, 500, 600, 700, 800]\n",
    "for seq_len in sequence_lengths:\n",
    "    run_experiment(seq_len)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
