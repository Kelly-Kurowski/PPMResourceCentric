{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faddb35-5881-4490-abdd-23a655ef15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    return pm4py.convert_to_dataframe(log)\n",
    "\n",
    "event_log = import_xes(\"BPI_Challenge_2019.xes\")\n",
    "df = event_log[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "df = df.sort_values(by=['org:resource', 'time:timestamp']).reset_index(drop=True)\n",
    "print(f\"‚úÖ Log loaded: {len(df)} events, {df['org:resource'].nunique()} resources.\")\n",
    "\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "    for resource, resource_df in df.groupby('org:resource'):\n",
    "        acts = resource_df['concept:name'].values\n",
    "        if len(acts) >= prefix_length + 1:\n",
    "            sequences.append(acts[:prefix_length])\n",
    "            next_activities.append(acts[prefix_length])\n",
    "            resources.append(resource)\n",
    "    if not sequences:\n",
    "        return pd.DataFrame()\n",
    "    seq_df = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    seq_df['next_activity'] = next_activities\n",
    "    seq_df['org:resource'] = resources\n",
    "    return seq_df\n",
    "\n",
    "def proportional_sampling(X, y):\n",
    "    unique_classes, counts = np.unique(y, return_counts=True)\n",
    "    max_count = counts.max()\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        if cls == -1:\n",
    "            continue\n",
    "        cls_mask = (y == cls)\n",
    "        X_cls, y_cls = X[cls_mask], y[cls_mask]\n",
    "        n_repeat = int(np.ceil(max_count / count))\n",
    "        X_resampled.append(np.tile(X_cls, (n_repeat, 1)))\n",
    "        y_resampled.append(np.tile(y_cls, n_repeat))\n",
    "    X_bal = np.vstack(X_resampled)\n",
    "    y_bal = np.hstack(y_resampled)\n",
    "    idx = np.random.permutation(len(y_bal))\n",
    "    return X_bal[idx], y_bal[idx]\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size//2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size//2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=15, patience=3, trial_num=None):\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    best_model_wts = model.state_dict()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            if (y_batch >= 0).sum() == 0:\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_count = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                mask = y_batch >= 0\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                output = model(X_batch)\n",
    "                val_loss += criterion(output, y_batch).item()\n",
    "                val_count += 1\n",
    "        val_loss /= max(val_count, 1)\n",
    "\n",
    "        print(f\"Trial {trial_num} | Epoch {epoch+1}/{epochs} | Train Loss: {total_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_wts = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_experiment(prefix_length):\n",
    "    print(f\"\\n Running experiment: sequence length = {prefix_length}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    sequences_df = create_activity_sequences(df, prefix_length)\n",
    "    if sequences_df.empty:\n",
    "        print(\"‚ö†Ô∏è Not enough sequences, skipping.\")\n",
    "        return\n",
    "\n",
    "    # Label encoding (fit only on training)\n",
    "    cols = [f\"activity_{i+1}\" for i in range(prefix_length)] + ['next_activity']\n",
    "    all_acts = sequences_df[cols].values.flatten()\n",
    "    global_le = LabelEncoder()\n",
    "    global_le.fit(all_acts)\n",
    "\n",
    "    for col in cols:\n",
    "        sequences_df[col] = global_le.transform(sequences_df[col])\n",
    "\n",
    "    X = sequences_df[[f\"activity_{i+1}\" for i in range(prefix_length)]].values.astype(np.float32)\n",
    "    y = sequences_df['next_activity'].values.astype(np.int64)\n",
    "\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    rare_labels = unique[counts < 2]\n",
    "    y = np.where(np.isin(y, rare_labels), -1, y)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train_res, y_train_res = proportional_sampling(X_train, y_train)\n",
    "    X_train_res = np.expand_dims(X_train_res, axis=2)\n",
    "    X_val = np.expand_dims(X_val, axis=2)\n",
    "\n",
    "    valid_classes = np.unique(y_train_res[y_train_res >= 0])\n",
    "    num_classes = len(valid_classes)\n",
    "    label_map = {old: new for new, old in enumerate(valid_classes)}\n",
    "    y_train_res = np.array([label_map[v] if v >= 0 else -1 for v in y_train_res])\n",
    "    y_val = np.array([label_map[v] if v in label_map else -1 for v in y_val])\n",
    "\n",
    "    def objective(trial):\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 32, 192, step=32)\n",
    "        lr = trial.suggest_categorical(\"lr\", [1e-4, 5e-4, 1e-3])\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"])\n",
    "        trial_num = trial.number + 1\n",
    "\n",
    "        print(f\"\\nüî• Starting Trial {trial_num} | hidden={hidden_size}, lr={lr}, opt={optimizer_name}\")\n",
    "\n",
    "        model = LSTMModel(X_train_res.shape[2], hidden_size, num_classes).to(device)\n",
    "        if optimizer_name == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        elif optimizer_name == \"rmsprop\":\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(X_train_res), torch.tensor(y_train_res)), batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(torch.tensor(X_val), torch.tensor(y_val)), batch_size=32)\n",
    "        model = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=15, patience=3, trial_num=trial_num)\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        valid_mask = y_val >= 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, _ in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                output = model(X_batch)\n",
    "                y_pred.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "        if valid_mask.sum() == 0:\n",
    "            return 0.0\n",
    "        return accuracy_score(y_val[valid_mask], np.array(y_pred)[valid_mask])\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=15, catch=(Exception,))\n",
    "\n",
    "    if len(study.trials) == 0 or study.best_trial is None:\n",
    "        print(\"‚ö†Ô∏è No successful trials. Skipping saving metrics.\")\n",
    "        return\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"üèÜ Best hyperparameters: {best_params}\")\n",
    "\n",
    "    hidden_size = best_params[\"hidden_size\"]\n",
    "    lr = best_params[\"lr\"]\n",
    "    optimizer_name = best_params[\"optimizer\"]\n",
    "\n",
    "    model = LSTMModel(X_train_res.shape[2], hidden_size, num_classes).to(device)\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_train_res), torch.tensor(y_train_res)), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.tensor(X_val), torch.tensor(y_val)), batch_size=32)\n",
    "    model = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=50, patience=5, trial_num=\"final\")\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    valid_mask = y_val >= 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            y_pred.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_val[valid_mask], np.array(y_pred)[valid_mask]),\n",
    "        \"precision\": precision_score(y_val[valid_mask], np.array(y_pred)[valid_mask], average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(y_val[valid_mask], np.array(y_pred)[valid_mask], average=\"weighted\", zero_division=0),\n",
    "        \"f1_score\": f1_score(y_val[valid_mask], np.array(y_pred)[valid_mask], average=\"weighted\", zero_division=0),\n",
    "        \"num_samples\": len(y_train_res),\n",
    "        \"sequence_length\": prefix_length\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        \"sequence_length\": prefix_length,\n",
    "        \"best_hyperparameters\": best_params,\n",
    "        \"metrics\": metrics,\n",
    "        \"runtime_seconds\": round(time.time() - start_time, 2)\n",
    "    }\n",
    "\n",
    "    os.makedirs(\"results/BPIC2019/LSTM model/Baseline encoding\", exist_ok=True)\n",
    "    out_path = f\"results/BPIC2019/LSTM model/Baseline encoding/lstm_seq_{prefix_length}.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"\\nüìä Evaluation Results for seq={prefix_length}:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"   {k}: {v:.4f}\")\n",
    "    print(f\"üíæ Results saved to {out_path}\")\n",
    "    print(f\"‚úÖ Experiment completed in {results['runtime_seconds']}s.\\n\")\n",
    "\n",
    "sequence_lengths = [100, 150, 200, 300, 400, 500, 600, 700, 800]\n",
    "for seq_len in sequence_lengths:\n",
    "    run_experiment(seq_len)\n",
    "\n",
    "print(\"All experiments completed! Results saved in /results/BPIC2019/LSTM model/Baseline encoding/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e219ac1-0823-4a7a-9596-f7e724f373d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCap\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    return pm4py.convert_to_dataframe(log)\n",
    "\n",
    "event_log = import_xes(\"BPI Challenge 2018.xes\")\n",
    "df = event_log[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "df = df.sort_values(by=['org:resource', 'time:timestamp']).reset_index(drop=True)\n",
    "\n",
    "def create_diversity_matrix(log):\n",
    "    activity_counts = log.pivot_table(\n",
    "        index='org:resource',\n",
    "        columns='concept:name',\n",
    "        aggfunc='size',\n",
    "        fill_value=0\n",
    "    )\n",
    "    activity_counts.reset_index(inplace=True)\n",
    "    return activity_counts\n",
    "\n",
    "ra_div_matrix = create_diversity_matrix(event_log)\n",
    "ra_div_binary = ra_div_matrix.copy()\n",
    "ra_div_binary.iloc[:, 1:] = (ra_div_binary.iloc[:, 1:] > 0).astype(int)\n",
    "activities = ra_div_matrix.columns[1:].tolist()\n",
    "\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "    for resource, resource_df in df.groupby('org:resource'):\n",
    "        acts = resource_df['concept:name'].values\n",
    "        if len(acts) >= prefix_length + 1:\n",
    "            sequences.append(acts[:prefix_length])\n",
    "            next_activities.append(acts[prefix_length])\n",
    "            resources.append(resource)\n",
    "    sequences_df = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    sequences_df['next_activity'] = next_activities\n",
    "    sequences_df['org:resource'] = resources\n",
    "    return sequences_df\n",
    "\n",
    "def proportional_sampling(X, y):\n",
    "    unique_classes, counts = np.unique(y, return_counts=True)\n",
    "    max_count = counts.max()\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        if cls == -1:\n",
    "            continue  # skip ignored label\n",
    "        cls_mask = (y == cls)\n",
    "        X_cls, y_cls = X[cls_mask], y[cls_mask]\n",
    "        n_repeat = int(np.ceil(max_count / count))\n",
    "        X_resampled.append(np.tile(X_cls, (n_repeat, 1)))\n",
    "        y_resampled.append(np.tile(y_cls, n_repeat))\n",
    "    X_bal = np.vstack(X_resampled)\n",
    "    y_bal = np.hstack(y_resampled)\n",
    "    idx = np.random.permutation(len(y_bal))\n",
    "    return X_bal[idx], y_bal[idx]\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size//2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size//2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=15, patience=3):\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    best_model_wts = model.state_dict()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            if (y_batch >= 0).sum() == 0:\n",
    "                continue  # skip batch if all labels are -1\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_count = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                mask = y_batch >= 0\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                output = model(X_batch)\n",
    "                val_loss += criterion(output, y_batch).item()\n",
    "                val_count += 1\n",
    "        val_loss = val_loss / max(val_count, 1)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_wts = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def run_experiment(prefix_length):\n",
    "    print(f\"\\nüöÄ Running experiment: sequence length = {prefix_length}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sequences_df = create_activity_sequences(df, prefix_length)\n",
    "    if sequences_df.empty:\n",
    "        print(f\"‚ö†Ô∏è Skipping sequence length {prefix_length}: not enough data\")\n",
    "        return\n",
    "    \n",
    "    ra_filtered = ra_div_binary[ra_div_matrix['org:resource'].isin(sequences_df['org:resource'])].reset_index(drop=True)\n",
    "    merged_df = pd.concat([sequences_df.reset_index(drop=True), ra_filtered.iloc[:, 1:]], axis=1)\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    all_acts = merged_df[[f\"activity_{i+1}\" for i in range(prefix_length)] + ['next_activity']].values.flatten()\n",
    "    label_encoder.fit(all_acts)\n",
    "    for col in [f\"activity_{i+1}\" for i in range(prefix_length)] + ['next_activity']:\n",
    "        merged_df[col] = label_encoder.transform(merged_df[col])\n",
    "    \n",
    "    X = merged_df[[f\"activity_{i+1}\" for i in range(prefix_length)] + activities].values.astype(np.float32)\n",
    "    y = merged_df['next_activity'].values.astype(np.int64)\n",
    "    \n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    rare_labels = unique[counts < 2]\n",
    "    y = np.where(np.isin(y, rare_labels), -1, y)\n",
    "    \n",
    "    X, y = proportional_sampling(X, y)\n",
    "    X = np.expand_dims(X, axis=2)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    valid_classes = np.unique(y[y >= 0])\n",
    "    num_classes = len(valid_classes)\n",
    "    label_map = {old: new for new, old in enumerate(valid_classes)}\n",
    "    y_train = np.array([label_map[v] if v >= 0 else -1 for v in y_train])\n",
    "    y_val = np.array([label_map[v] if v >= 0 else -1 for v in y_val])\n",
    "    \n",
    "    def objective(trial):\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 32, 192, step=32)\n",
    "        lr = trial.suggest_categorical(\"lr\", [1e-4, 5e-4, 1e-3])\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"])\n",
    "        \n",
    "        model = LSTMModel(X_train.shape[2], hidden_size, num_classes).to(device)\n",
    "        if optimizer_name == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        elif optimizer_name == \"rmsprop\":\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)), batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(torch.tensor(X_val), torch.tensor(y_val)), batch_size=32)\n",
    "        \n",
    "        model = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=15, patience=3)\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        valid_mask = y_val >= 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                output = model(X_batch)\n",
    "                y_pred.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "        if valid_mask.sum() == 0:\n",
    "            return 0.0\n",
    "        acc = accuracy_score(y_val[valid_mask], np.array(y_pred)[valid_mask])\n",
    "        return acc\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=15, catch=(Exception,))\n",
    "    \n",
    "    if len(study.trials) == 0 or study.best_trial is None:\n",
    "        print(\"‚ö†Ô∏è No successful trials. Skipping saving metrics.\")\n",
    "        return\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    print(f\"üèÜ Best hyperparameters for seq {prefix_length}: {best_params}\")\n",
    "    \n",
    "    hidden_size = best_params[\"hidden_size\"]\n",
    "    lr = best_params[\"lr\"]\n",
    "    optimizer_name = best_params[\"optimizer\"]\n",
    "    model = LSTMModel(X_train.shape[2], hidden_size, num_classes).to(device)\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.tensor(X_val), torch.tensor(y_val)), batch_size=32)\n",
    "    model = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=50, patience=5)\n",
    "    \n",
    "    # Metrics\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    valid_mask = y_val >= 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            y_pred.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_val[valid_mask], np.array(y_pred)[valid_mask]),\n",
    "        \"precision\": precision_score(y_val[valid_mask], np.array(y_pred)[valid_mask], average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(y_val[valid_mask], np.array(y_pred)[valid_mask], average=\"weighted\", zero_division=0),\n",
    "        \"f1_score\": f1_score(y_val[valid_mask], np.array(y_pred)[valid_mask], average=\"weighted\", zero_division=0),\n",
    "        \"num_samples\": len(y_train),\n",
    "        \"sequence_length\": prefix_length\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        \"sequence_length\": prefix_length,\n",
    "        \"best_hyperparameters\": best_params,\n",
    "        \"metrics\": metrics,\n",
    "        \"runtime_seconds\": round(time.time() - start_time, 2)\n",
    "    }\n",
    "\n",
    "    os.makedirs(\"results/BPIC2018/LSTM model/SCap_check\", exist_ok=True)\n",
    "    out_path = f\"results/BPIC2018/LSTM model/SCap_check/lstm_seq_{prefix_length}.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"\\nüíæ Results saved to {out_path}\")\n",
    "    print(f\"‚úÖ Experiment for sequence_length={prefix_length} completed in {results['runtime_seconds']}s.\\n\")\n",
    "\n",
    "sequence_lengths = [2500]\n",
    "for seq_len in sequence_lengths:\n",
    "    run_experiment(seq_len)\n",
    "\n",
    "print(\"\\nüéØ All experiments completed! Results saved in /LSTM model/SCap/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f723fcd-2962-405b-89c0-3264dbe8a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S2g\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    return pm4py.convert_to_dataframe(log)\n",
    "\n",
    "event_log = import_xes(\"BPI Challenge 2018.xes\")\n",
    "df = event_log[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "df = df.sort_values(by=['org:resource', 'time:timestamp']).reset_index(drop=True)\n",
    "print(f\"‚úÖ Log loaded: {len(df)} events, {df['org:resource'].nunique()} resources.\")\n",
    "\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "    for resource, group in df.groupby('org:resource'):\n",
    "        acts = group['concept:name'].tolist()\n",
    "        if len(acts) > prefix_length:\n",
    "            sequences.append(acts[:prefix_length])\n",
    "            next_activities.append(acts[prefix_length])\n",
    "            resources.append(resource)\n",
    "    if not sequences:\n",
    "        return pd.DataFrame()\n",
    "    seq_df = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    seq_df[\"next_activity\"] = next_activities\n",
    "    seq_df[\"org:resource\"] = resources\n",
    "    return seq_df\n",
    "\n",
    "def create_transition_features(sequences_df):\n",
    "    unique_activities = sorted(set(sequences_df.drop(columns=['next_activity','org:resource']).values.flatten()))\n",
    "    all_transitions = [(a, b) for a in unique_activities for b in unique_activities]\n",
    "    transition_counts = []\n",
    "    for _, row in sequences_df.iterrows():\n",
    "        transitions = defaultdict(int)\n",
    "        acts = row.drop(labels=['next_activity','org:resource']).dropna().tolist()\n",
    "        for i in range(len(acts)-1):\n",
    "            transitions[(acts[i], acts[i+1])] += 1\n",
    "        row_counts = {f\"{a}->{b}\": transitions.get((a,b),0) for (a,b) in all_transitions}\n",
    "        transition_counts.append(row_counts)\n",
    "    return pd.concat([sequences_df.reset_index(drop=True), pd.DataFrame(transition_counts)], axis=1)\n",
    "\n",
    "def proportional_sampling(X, y):\n",
    "    unique_classes, counts = np.unique(y, return_counts=True)\n",
    "    max_count = counts.max()\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        cls_mask = (y == cls)\n",
    "        X_cls, y_cls = X[cls_mask], y[cls_mask]\n",
    "        n_repeat = int(np.ceil(max_count / count))\n",
    "        X_resampled.append(np.tile(X_cls, (n_repeat, 1)))\n",
    "        y_resampled.append(np.tile(y_cls, n_repeat))\n",
    "    X_bal = np.vstack(X_resampled)\n",
    "    y_bal = np.hstack(y_resampled)\n",
    "    idx = np.random.permutation(len(y_bal))\n",
    "    return X_bal[idx], y_bal[idx]\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size // 2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size // 2, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=15, patience=3, trial_num=None):\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    best_wts = model.state_dict()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                output = model(X_batch)\n",
    "                val_loss += criterion(output, y_batch).item()\n",
    "                count += 1\n",
    "        val_loss /= max(count, 1)\n",
    "\n",
    "        print(f\"üåÄ Trial {trial_num} | Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_wts = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"‚ö†Ô∏è Early stopping at epoch {epoch+1} (no improvement).\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_wts)\n",
    "    return model\n",
    "\n",
    "def run_experiment(prefix_length):\n",
    "    print(f\"\\nüöÄ Running experiment for prefix_length = {prefix_length}\")\n",
    "    start_time = time.time()\n",
    "    sequences_df = create_activity_sequences(df, prefix_length)\n",
    "    if sequences_df.empty:\n",
    "        print(f\"‚ö†Ô∏è Skipping prefix_length={prefix_length}: not enough data.\")\n",
    "        return\n",
    "    sequences_df = create_transition_features(sequences_df)\n",
    "\n",
    "    # Encode all object columns\n",
    "    encoders = {}\n",
    "    for col in sequences_df.columns:\n",
    "        if sequences_df[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            sequences_df[col] = le.fit_transform(sequences_df[col].astype(str))\n",
    "            encoders[col] = le\n",
    "\n",
    "    # Split features/labels\n",
    "    X = sequences_df.drop(columns=['next_activity', 'org:resource']).values.astype(np.float32)\n",
    "    y = sequences_df['next_activity'].values.astype(str)  # keep as string for LabelEncoder\n",
    "    X = np.expand_dims(X, axis=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Fit LabelEncoder on training labels only\n",
    "    le_target = LabelEncoder()\n",
    "    y_train_enc = le_target.fit_transform(y_train)\n",
    "\n",
    "    # Filter validation to only seen classes\n",
    "    valid_mask = np.isin(y_val, le_target.classes_)\n",
    "    X_val = X_val[valid_mask]\n",
    "    y_val = y_val[valid_mask]\n",
    "    y_val_enc = le_target.transform(y_val)\n",
    "\n",
    "    # Oversample training only\n",
    "    X_train_flat = X_train.reshape((X_train.shape[0], X_train.shape[2]))\n",
    "    X_train_res, y_train_res = proportional_sampling(X_train_flat, y_train_enc)\n",
    "    X_train_res = np.expand_dims(X_train_res, axis=1)\n",
    "\n",
    "    num_classes = len(le_target.classes_)\n",
    "\n",
    "    # Optuna Hyperparameter Tuning\n",
    "    def objective(trial):\n",
    "        trial_num = trial.number + 1\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 32, 192, step=32)\n",
    "        lr = trial.suggest_categorical(\"lr\", [1e-4, 5e-4, 1e-3])\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"])\n",
    "\n",
    "        print(f\"\\nüî• Starting Trial {trial_num} | hidden={hidden_size}, lr={lr}, opt={optimizer_name}\")\n",
    "\n",
    "        model = LSTMModel(X_train_res.shape[2], hidden_size, num_classes).to(device)\n",
    "        if optimizer_name == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        elif optimizer_name == \"rmsprop\":\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(X_train_res), torch.tensor(y_train_res)), batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(torch.tensor(X_val), torch.tensor(y_val_enc)), batch_size=32)\n",
    "        model = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=15, patience=3, trial_num=trial_num)\n",
    "\n",
    "        # Evaluate accuracy\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, _ in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                output = model(X_batch)\n",
    "                y_pred.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "        acc = accuracy_score(y_val_enc, y_pred)\n",
    "        print(f\"‚úÖ Trial {trial_num} finished with Accuracy: {acc:.4f}\")\n",
    "        return acc\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=15, catch=(Exception,))\n",
    "    if len(study.trials) == 0 or study.best_trial is None:\n",
    "        print(\"‚ö†Ô∏è No successful trials. Skipping saving metrics.\")\n",
    "        return\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"\\nüèÜ Best Hyperparameters for seq {prefix_length}: {best_params}\")\n",
    "\n",
    "    # Final Training with Best Params\n",
    "    hidden_size = best_params[\"hidden_size\"]\n",
    "    lr = best_params[\"lr\"]\n",
    "    optimizer_name = best_params[\"optimizer\"]\n",
    "\n",
    "    model = LSTMModel(X_train_res.shape[2], hidden_size, num_classes).to(device)\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_train_res), torch.tensor(y_train_res)), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.tensor(X_val), torch.tensor(y_val_enc)), batch_size=32)\n",
    "    model = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=50, patience=5, trial_num=\"final\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            y_pred.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_val_enc, y_pred),\n",
    "        \"precision\": precision_score(y_val_enc, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(y_val_enc, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"f1_score\": f1_score(y_val_enc, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"num_samples\": len(y_train_res),\n",
    "        \"sequence_length\": prefix_length\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        \"sequence_length\": prefix_length,\n",
    "        \"best_hyperparameters\": best_params,\n",
    "        \"metrics\": metrics,\n",
    "        \"runtime_seconds\": round(time.time() - start_time, 2)\n",
    "    }\n",
    "\n",
    "    os.makedirs(\"results/BPIC2018/LSTM model/S2g\", exist_ok=True)\n",
    "    out_path = f\"results/BPIC2018/LSTM model/S2g/lstm_seq_{prefix_length}.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(\"\\nüìä Evaluation Results:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"   {k}: {v:.4f}\")\n",
    "    print(f\"üíæ Results saved to {out_path}\")\n",
    "    print(f\"‚úÖ Experiment for sequence_length={prefix_length} completed in {results['runtime_seconds']}s.\\n\")\n",
    "\n",
    "sequence_lengths = [100, 150, 200, 400, 600, 800, 1000, 1200, 1400, 1500, 2000, 2500]\n",
    "for seq_len in sequence_lengths:\n",
    "    run_experiment(seq_len)\n",
    "\n",
    "print(\"\\nüéØ All experiments completed! Results saved in /results/BPIC2019/LSTM model/S2g_fixed/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c67db3-c806-48eb-8141-6073f9f5db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S2gR\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import optuna\n",
    "import pm4py\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    return pm4py.convert_to_dataframe(log)\n",
    "\n",
    "event_log = import_xes(\"BPI_Challenge_2013_incidents.xes\")\n",
    "df = event_log[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "df = df.sort_values(by=['org:resource', 'time:timestamp']).reset_index(drop=True)\n",
    "print(f\"‚úÖ Log loaded: {len(df)} events, {df['org:resource'].nunique()} resources.\")\n",
    "\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "    for resource, group in df.groupby('org:resource'):\n",
    "        acts = group['concept:name'].tolist()\n",
    "        if len(acts) > prefix_length:\n",
    "            sequences.append(acts[:prefix_length])\n",
    "            next_activities.append(acts[prefix_length])\n",
    "            resources.append(resource)\n",
    "    if not sequences:\n",
    "        return pd.DataFrame()\n",
    "    seq_df = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    seq_df[\"next_activity\"] = next_activities\n",
    "    seq_df[\"org:resource\"] = resources\n",
    "    return seq_df\n",
    "\n",
    "def create_transition_and_repeat_features(sequences_df):\n",
    "    unique_activities = sorted(set(sequences_df.drop(columns=['next_activity','org:resource']).values.flatten()))\n",
    "    all_transitions = [(a,b) for a in unique_activities for b in unique_activities]\n",
    "\n",
    "    transition_counts = []\n",
    "    repeat_features = []\n",
    "\n",
    "    for _, row in sequences_df.iterrows():\n",
    "        transitions = defaultdict(int)\n",
    "        acts = row.drop(labels=['next_activity','org:resource']).dropna().tolist()\n",
    "\n",
    "        # Count transitions\n",
    "        for i in range(len(acts)-1):\n",
    "            transitions[(acts[i], acts[i+1])] += 1\n",
    "        row_counts = {f\"{a}->{b}\": transitions.get((a,b),0) for (a,b) in all_transitions}\n",
    "        transition_counts.append(row_counts)\n",
    "\n",
    "        # Repeat pattern features\n",
    "        current_run = 1\n",
    "        run_lengths = []\n",
    "        for i in range(1, len(acts)):\n",
    "            if acts[i] == acts[i-1]:\n",
    "                current_run += 1\n",
    "            else:\n",
    "                run_lengths.append(current_run)\n",
    "                current_run = 1\n",
    "        run_lengths.append(current_run)\n",
    "        repeat_features.append({\n",
    "            \"avg_run_length\": np.mean(run_lengths),\n",
    "            \"num_runs\": len(run_lengths)\n",
    "        })\n",
    "\n",
    "    transitions_df = pd.DataFrame(transition_counts)\n",
    "    repeat_df = pd.DataFrame(repeat_features)\n",
    "    return pd.concat([sequences_df.reset_index(drop=True), transitions_df, repeat_df], axis=1)\n",
    "\n",
    "def proportional_sampling(X, y):\n",
    "    unique_classes, counts = np.unique(y, return_counts=True)\n",
    "    max_count = counts.max()\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls, count in zip(unique_classes, counts):\n",
    "        cls_mask = (y == cls)\n",
    "        X_cls, y_cls = X[cls_mask], y[cls_mask]\n",
    "        n_repeat = int(np.ceil(max_count / count))\n",
    "        X_resampled.append(np.tile(X_cls, (n_repeat, 1)))\n",
    "        y_resampled.append(np.tile(y_cls, n_repeat))\n",
    "    X_bal = np.vstack(X_resampled)\n",
    "    y_bal = np.hstack(y_resampled)\n",
    "    idx = np.random.permutation(len(y_bal))\n",
    "    return X_bal[idx], y_bal[idx]\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size//2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size//2, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=15, patience=3, trial_num=None):\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    best_wts = model.state_dict()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                output = model(X_batch)\n",
    "                val_loss += criterion(output, y_batch).item()\n",
    "                count += 1\n",
    "        val_loss /= max(count, 1)\n",
    "\n",
    "        print(f\"üåÄ Trial {trial_num} | Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_wts = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"‚ö†Ô∏è Early stopping at epoch {epoch+1} (no improvement).\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_wts)\n",
    "    return model\n",
    "\n",
    "def run_experiment(prefix_length):\n",
    "    print(f\"Running Experiment 4 for prefix_length = {prefix_length}\")\n",
    "    start_time = time.time()\n",
    "    sequences_df = create_activity_sequences(df, prefix_length)\n",
    "    if sequences_df.empty:\n",
    "        print(f\"‚ö†Ô∏è Skipping prefix_length={prefix_length}: not enough data.\")\n",
    "        return\n",
    "    sequences_df = create_transition_and_repeat_features(sequences_df)\n",
    "\n",
    "    # Encode object features except target\n",
    "    for col in sequences_df.columns:\n",
    "        if sequences_df[col].dtype == 'object' and col not in ['next_activity']:\n",
    "            le = LabelEncoder()\n",
    "            sequences_df[col] = le.fit_transform(sequences_df[col].astype(str))\n",
    "\n",
    "    # Separate features and raw labels\n",
    "    X = sequences_df.drop(columns=['next_activity','org:resource']).values.astype(np.float32)\n",
    "    y_raw = sequences_df['next_activity'].astype(str).values\n",
    "\n",
    "    # Split before encoding y\n",
    "    X_train, X_val, y_train_raw, y_val_raw = train_test_split(X, y_raw, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Encode y only on training labels\n",
    "    le_y = LabelEncoder()\n",
    "    y_train = le_y.fit_transform(y_train_raw)\n",
    "    y_val = np.array([le_y.transform([label])[0] if label in le_y.classes_ else -1 for label in y_val_raw])\n",
    "    mask = y_val != -1\n",
    "    X_val = X_val[mask]\n",
    "    y_val = y_val[mask]\n",
    "\n",
    "    # Balance only the training data\n",
    "    X_train, y_train = proportional_sampling(X_train, y_train)\n",
    "    X_train = np.expand_dims(X_train, axis=1)\n",
    "    X_val = np.expand_dims(X_val, axis=1)\n",
    "    num_classes = len(le_y.classes_)\n",
    "\n",
    "    def objective(trial):\n",
    "        trial_num = trial.number + 1\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 32, 192, step=32)\n",
    "        lr = trial.suggest_categorical(\"lr\", [1e-4, 5e-4, 1e-3])\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"])\n",
    "\n",
    "        print(f\"\\nüî• Starting Trial {trial_num} | hidden={hidden_size}, lr={lr}, opt={optimizer_name}\")\n",
    "\n",
    "        model = LSTMModel(X_train.shape[2], hidden_size, num_classes).to(device)\n",
    "        if optimizer_name == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        elif optimizer_name == \"rmsprop\":\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)), batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(torch.tensor(X_val), torch.tensor(y_val)), batch_size=32)\n",
    "        model = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=15, patience=3, trial_num=trial_num)\n",
    "\n",
    "        # Evaluate accuracy\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, _ in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                output = model(X_batch)\n",
    "                y_pred.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        print(f\"‚úÖ Trial {trial_num} finished with Accuracy: {acc:.4f}\")\n",
    "        return acc\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=18, catch=(Exception,))\n",
    "    if len(study.trials) == 0 or study.best_trial is None:\n",
    "        print(\"‚ö†Ô∏è No successful trials. Skipping saving metrics.\")\n",
    "        return\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"Best Hyperparameters for seq {prefix_length}: {best_params}\")\n",
    "\n",
    "    hidden_size = best_params[\"hidden_size\"]\n",
    "    lr = best_params[\"lr\"]\n",
    "    optimizer_name = best_params[\"optimizer\"]\n",
    "\n",
    "    model = LSTMModel(X_train.shape[2], hidden_size, num_classes).to(device)\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.tensor(X_val), torch.tensor(y_val)), batch_size=32)\n",
    "    model = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=50, patience=5, trial_num=\"final\")\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            y_pred.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_val, y_pred),\n",
    "        \"precision\": precision_score(y_val, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(y_val, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"f1_score\": f1_score(y_val, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"num_samples\": len(y_train),\n",
    "        \"sequence_length\": prefix_length\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        \"sequence_length\": prefix_length,\n",
    "        \"best_hyperparameters\": best_params,\n",
    "        \"metrics\": metrics,\n",
    "        \"runtime_seconds\": round(time.time() - start_time, 2)\n",
    "    }\n",
    "\n",
    "    os.makedirs(\"results/BPIC2013/LSTM model/S2gR\", exist_ok=True)\n",
    "    out_path = f\"results/BPIC2013/LSTM model/S2gR/lstm_seq_{prefix_length}.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(\"\\nüìä Evaluation Results:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"   {k}: {v:.4f}\")\n",
    "    print(f\"üíæ Results saved to {out_path}\")\n",
    "    print(f\"‚úÖ Experiment for sequence_length={prefix_length} completed in {results['runtime_seconds']}s.\\n\")\n",
    "\n",
    "sequence_lengths = [10, 20, 30, 40, 50, 75, 100, 125, 150]\n",
    "for seq_len in sequence_lengths:\n",
    "    run_experiment(seq_len)\n",
    "\n",
    "print(\"\\nüéâ All experiments completed successfully for Experiment 4!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
